Go-Explore：のための新しいアプローチ 問題
 エイドリアン・エコフェット Joost Huizinga ジョエル・リーマン ケネス・O・スタンレー* Jeff Clune * ユーバーAIラボ サンフランシスコ、CA 94103 adrienle、jhuizinga、joel.lehman、kstanley、jeffclune @ uber.com *共著者

Abstract
強化学習における大きな課題は知的な探索です。特に報酬がまばらかdecdptive(詐欺的)であるときに起こります。ある2つのAtariゲームがそのような困難な探査領域のベンチマークとして機能する。そのゲームは「モンテスマの復讐」と「落とし穴」である。 両方のゲームで、現在のRLアルゴリズムは、内発的な動機づけ（hard-exploration分野において探査を促し、ハードウェアの性能を向上させる支配的なアルゴリズム）の手法でさえ、パフォーマンスが芳しくない。
この問題に対処するために、新しいアルゴリズム「Go-Explore」を紹介します。
それは以下の原則を主に使います。
（1）すでに訪れた状態を記録しておく
（2）記録しておいた場所に（探索をせずに）戻ってそれから探索する。
（3）決定論を導入する等、利用可能な手段を使って模擬環境を解く。それから模倣学習を介して、確実に解決策を実行できる戦略を作成する。
これらの原則を組み合わせた結果、hard-explorationのパフォーマンスが劇的に向上した。モンテズマの復讐では、ドメインの知識使わない場合であっても、Go-Exploreのスコアは43,000ポイント以上だった。これは最先技術のほぼ4倍である。Go-Exploreは人間が用意したドメインの知識も簡単に利用することができ、それを用いたときスコアが平均650000以上になります。最大では約1800万にもなり人間の世界記録を一桁超えている。これは「超人的」とも言える。落とし穴では、ドメイン知識を使ったGo-Exploreは、ゼロ以上のスコアを獲得した最初のアルゴリズムです。その平均性能は 60,000ポイント近くで、人間の能力を上回っている。
Exploreは多くの高性能デモを自動的に安く作成でき、それは人間のデモンストレーションの形で提供されたそれ以前の模倣学習をしのぐ性能である。Go-Exploreは新しい研究において、それを向上させ、現在の強化学習アルゴリズムへ組み込むことのできる可能性を拡大させます。（意訳）
それはまた、以前は解決できなかったhard-exploration分野を向上させるかもしれない。特にロボットなどのシミュレータを利用することが多い分野の訓練等。

1. Introduction

強化学習（RL）は近年大きな進歩を遂げています。 GoのようなボードゲームやAtariのような古典的なビデオゲームにおいて人間と同程度のパフォーマンスに達している。
しかしながら、このような進歩はあっても現実問題における難しい課題をRLは解決していない。（超意訳）
特に、報酬がsparse(まばら)かdeceptive（詐欺的）である環境を調査して解析するためには効果的な探索が要求される。
報酬がまばらであるときにおいては、次の報酬までに正確な順番で多くの（数百以上かも）行動をとらなければならない。
報酬が詐欺的であるときはさらに難しい。報酬が少数ではない代わりに、最後まで到達するための報酬が間違って提供されるからだ。
それのせいで現在の場所に固執してはまってしまうことにつながってしまう。
その報酬がまばらで、詐欺的な問題両方のことをhard-exploration問題といい、従来のRLアルゴリズムのパフォーマンスは低い。
残念ながら、現実世界での問題はhard-exploration問題に属する。
これは現実問題が抽象的なゴール（例えば、「生存者を探して、位置を発信する」、「リアクター内で漏れているパイプのバルブを閉める」など）を目指しているからだ。そしてこのような問題では解決のための詳細なガイドラインを設定できないどころか、間違っている（deception）条件を生み出す。

たとえば、被災地で被災者を見つける場合、被災者の数は少ないことが多い。このように希薄性をもたらすこともある。さらに悪いことに、もしロボットに自身への被害を最小限にするように指示したら、積極的に環境を探索しないようにロボットに教えることにつながる可能性もある。なぜなら探索の始めのうちは生存者を見つけ出すことよりも、被害をもたらされる可能性のほうがはるかに高いからだ。
このように報酬がまばらな環境においては、一見賢明な指示が最終的に間違いを教えていることになるかもしれない。

これらの課題に対処するためにこの論文では、Go-Exploreを紹介する。
これはhard-explorationのベンチマークであるモンテズマの復讐と落とし穴の2つにおいて、現在手法よりも高いスコアを獲得できる、まったく新しいアルゴリズムである。

Go-Exploreより以前のまばらな報酬問題に対する手法は内発的動機付け（IM）である。これは内発的報酬（IRs）（環境から得られる外発的報酬を増加させたり、置き換えたりした報酬）をエージェントに与えることで、探索を促すものである。IMは好奇心や新奇探索傾向といった人間の探索や学習の根源である、心理学的概念によって動機づけられている。IMによってまばらな報酬問題は良い方向に進歩した一方で、モンテズマの復讐と落とし穴を含む多くの分野で、まだ問題を解決するには程遠い手法である。我々はそのような失敗はどのような問題であっても、2つの根本的なものから生じると仮定する。その２つを分離（detachment）と脱線（derailment）と呼ぶこととする。



IMはまばらな報酬問題で刺激的な進歩を生み出しましたが、 多くの分野でIMアプローチはまだ問題を完全に解決するには程遠いです 復讐と落とし穴 私たちは、他の問題の中でも、そのような失敗は2つの根本から生じると仮定します。 我々は分離と脱線と呼ばれることを引き起こします。 デタッチメントは、IMによって動かされたエージェントが、のフロンティアから切り離される可能性があるという考えです。 高い内的報酬（IR）。 分離を理解するために、私たちは最初にその本質的な報酬を考慮しなければなりません ほとんど常に消耗品です。好奇心の強いエージェントは、それが可能な範囲で国家について好奇心が強いです。 彼らを訪ねたことはあまりありません（驚き、目新しさ、または予測エラーを求めることにも同様の議論が当てはまります） 剤[ 4、 14-16]）。 エージェントが高IRを生成する状態空間の複数の領域を発見した場合、 政策は短期的にはそのような分野の一つに焦点を当てるかもしれない。 それによって提供されるIRの一部を使い果たした後 地域によっては、その政策が偶然にも別の地域でIRを消費し始める可能性があります。 そのIRを使い果たしたら、それは 初期領域でそれが切り離されたフロンティアを再発見するのは難しいです。 そのフロンティアにつながったIRを消費して（図 1 ）、どのようにして戻るかを覚えていないでしょう。 壊滅的な忘却によるその最前線 [ 17–20]。 このプロセスが発生するたびに、潜在的な道 探査が失われたり、少なくとも再発見が困難になる可能性があります。 最悪の場合、 現在の政策によって訪問された州空間の区域の近くに残っているIRの不足 IRは他の場所に留まるかもしれません）、したがってエージェントをさらに進めるための学習シグナルは残っていません。 効果的で知識のある方法で探求する。 本質的な見返りを徐々に追加することができます。 しかし、その後、無駄なプロセス全体が無期限に繰り返される可能性があります。 理論的には、リプレイバッファによって防止できます。 分離は、実際には放棄されたフロンティアに関するデータを防ぐために大きくする必要があります。 必要になる前にパージしないでください、そして大きい再生バッファはそれら自身の最適化を導入します 安定性の問題 [ 21、22]。 Go-Exploreアルゴリズムは、明示的に格納することによってデタッチに対処します 有望な州のアーカイブが訪問されたので、後で再び訪れて調査することができます。 エージェントが有望な状態を発見したときに脱線が発生する可能性があり、それは有益です その状態に戻り、そこから探検する。 典型的なRLアルゴリズムは、そのような望ましいことを実行しようとします 再び初期状態に至ったポリシーを実行することによる、しかしいくつかの確率論的摂動による行動 少し異なる行動を奨励するために、既存の方針が混ざり合っている（例：さらに探求する）。 の IMエージェントには2層の探査メカニズムがあるため、確率的摂動が実行されます。 （1）新しい州に到達したときに報酬を与える高レベルのIRインセンティブ、および（2）より基本的な イプシロン欲張り探索、アクション空間ノイズ、パラメータ空間などの探索メカニズム ノイズ [23 - 25]。 重要なことに、IMエージェントは後者のメカニズムに依存している状態を発見します。 高IR、そしてそれらに戻る前者のメカニズム。 しかし、より長く、より複雑に、そしてもっと 以前に発見された高IR状態に到達するためには、正確な一連のアクションが必要です。 そのような確率論的摂動は、エージェントがこれまでに戻ってこないように「脱線」させる可能性が高いです。 その状態。 それは必要とされる正確な行動が基本的な探査によって素朴に混乱させられるからです。 メカニズム、エージェントが引き寄せられる既知の状態に達することに成功することはめったにありません。 2
3ページ
図1：内発的動機付け（IM）アルゴリズムにおける分離の仮説的な例。 緑 領域は固有の報酬を示し、白は固有の報酬が残っていない領域を示し、紫色 領域は、アルゴリズムが現在探索している場所を示します。 （1）エージェントは、各エピソードを 2つの迷路 （2）偶然に西の迷路を探索し始めることがあり、IMはそれを学ぶためにそれを駆り立てることがある トラバース、例えば、それの50％。 （3）現在のアルゴリズムはランダム性をふりかけているため（アクションまたは （パラメータ））偶然にも、エージェントが偶然に明示的または内在的な報酬を見つけるために新しい行動を生み出すことを試みること ある時点で東の迷路を探索し始めるかもしれません、そこでそれはまた多くの固有の報酬に出会うでしょう。 東迷路を完全に探索した後、それは有望な探査の明確な記憶を持っていません フロンティアそれは西の迷路で放棄した。 それはまたこのフロンティアの暗黙の記憶がないだろう 壊滅的な忘却の問題のため[ 17–20 ]。 （4）最悪、最前線への道 西の迷路はすでに調査されているので、再発見するための固有の動機は（またはほとんど）残らない それ。 我々はこのように、アルゴリズムが本質的な動機を提供する状態のフロンティアから切り離したと言う。 その結果、現在のエージェントが訪れた場所の近くにあるエリアがすでに探索されている場合、探査は失速する可能性があります。 調べられた。 この問題は、エージェントが記憶して以前の状態に戻った場合に修復されます。 Go-Exploreが行っている探査に有望な分野を発見した。 フェーズ1：解決するまで探検する フェーズ2：強固化 （必要であれば） 州に行く 探検する 州から 更新 アーカイブ 模倣学習を実行する 最高の軌道で 状態を選択 アーカイブから 図2：Go-Exploreアルゴリズムの概要 そしてそこからさらに探査するのが最も効果的かもしれません。 脱線に対処するための洞察 Go-Exploreは、効果的な探査が最初に有望な状態に戻ることに分解される可能性があるということです。 その後さらに探索する前に（意図的に探索を追加することなく）。 Go-Exploreは、分離と脱線の両方に対する明示的な応答であり、これも達成するように設計されています。 確率的環境におけるロバスト解 ここで紹介するバージョンは2段階で機能します（図 2） 。 （1）最初に問題を解決するには、以下のようにします。 問題を解決する（つまり、問題を解決する方法を発見する） 確率論の存在下で確実に解を実行する。 1 IMアルゴリズムと同様、フェーズ1 まれにしか訪問されていない州の探索に焦点を当てています。これはまばらな報酬に対処するための基礎を形成します。 1 フェーズ2自体が対処可能なポリシーを作成する場合、この2番目のフェーズは原則として不要です。 確率的環境（セクション 2.1.3） 3
4ページ
そして詐欺的な問題。 IMアルゴリズムとは対照的に、フェーズ1では切り離しと脱線化について説明します。 国家のアーカイブと2つの戦略を通してそれらに到達する方法を蓄積することによって：（a）すべてを加える 興味深いことに、これまでにアーカイブにアクセスしたさまざまな状態、および（b）アーカイブからの状態ごとに 探索するために選択され、まずその状態に戻り（探索を追加せずに）、次に探索する 新しい州を探してその州からさらに離れた場所（したがって「Go-Explore」という名前）。 家を探すというアナロジーは、IMアルゴリズムとGo-Exploreのフェーズ1を対比させるのに役立ちます。 IMアルゴリズムは、家の中を懐中電灯で探すのと同じようなものです。 まず家のある場所、次に別の場所、そして別の場所などで探査を行います。 小さな可視領域の端にある固有の動機の領域に向けて描かれています。 であれば迷子になる可能性があります 本質的な動機が残っていても、ビームがどの領域にも当たらない任意の点。 もっと探索する 家のある部屋、その隣の部屋、そして隣の部屋の電灯をつけるのに似ています 家全体が照らされるまで部屋など。 Go-Exploreは徐々にその範囲を広げます 解決策が見つかるまで全方向の知識 必要に応じて、Go-Exploreの第2段階ではアーカイブから高性能の軌跡を削除します。 それらが本当の環境の確率論的ダイナミクスに対してロバストであるように。 Go-Exploreの強固さ 模倣学習（デモンストレーションまたはLfD [ 26 ] - [ 29] からの学習）を介して 、 人間のデモンストレーションからタスクを解決するため。 Go-Exploreとの唯一の違いは、その解決策です。 デモは、提供されるのではなく、Go-Exploreのフェーズ1によって自動的に作成されます。 人間による。 このフェーズへの入力は1つ以上の高性能軌道であり、出力は 一貫した同様のパフォーマンスを達成できる堅牢なポリシー。 両方のフェーズの組み合わせ 困難な探査問題のための強力なアルゴリズムを具体化します。 詐欺的報酬環境と高性能軌道を信頼性の高いソリューションにロバストにする 修正されていない確率的テスト環境でうまく機能する。 これらのアイデアのいくつかは、関連研究で提案されているアイデアと似ています。 それらの関係はで議論されています セクション 5。そうは言っても、私達は私達がこのようにこれらのアイデアを組み合わせて実証した最初の人であると信じています そうすることで、困難な探査の問題でパフォーマンスが大幅に向上します。 その可能性を探るために、我々はアーケードの2つの困難な探査ベンチマークでGo-Exploreをテストします。 学習環境（ALE）[ 30、31 ]：モンテスマの復讐と落とし穴。 モンテスマの復讐 探査アルゴリズム（固有の動機を含む）の重要なベンチマークとなっています アルゴリズム） [ 4、16、32–39]何百もの行動の正確なシーケンスを取り入れなければならないため 報酬を受け取る間。 落とし穴は、その報酬がまばらでないため、さらに難しくなります（32の正の値 報酬は255の部屋に散らばっています）そして多くの行動は小さな負の報酬を生み出すからです。 RLアルゴリズムが環境を探索しないようにします。 DQN [3 ]、A3C [40]、Ape- などの古典的なRLアルゴリズム（すなわち、固有の動機づけのないもの） X [ 41]およびIMPALA [42]は、最大220億のゲームフレームがあっても、これらのドメインではうまく機能しません。 経験値が、Montezumaの復讐のスコアが2,500以下で、レベル1の解決に失敗している。 Pitfallで0以下の得点。 これらの結果は、決定論的テストで評価された実験を除外しています 環境[ 43、44 ]または人間のデモンストレーションを与えられた[26、27、45]。 落とし穴では、 肯定的な報酬と頻繁な否定的な報酬によって、RLアルゴリズムは効果的なポリシーを学習します。 完全に静止しているか、ゲームの開始近くで前後に移動しても、何もしません。 （https://youtu.be/Z0lYamtgdqQ [ 46]）。 これら2つのゲームは、計画を許可されている場合でも、アルゴリズムを計画するのが非常に困難です。 ゲームエミュレータ内で直接。 UCTなどの古典的計画アルゴリズム [ 47–49]（強力な モンテカルロ木探索法 [49、50 ]）はモンテスマの復讐で0点を得る スペースも確率的方法[して、効率的に探索するには大きすぎる 30、 51]。 まばらな報酬問題に取り組むために特別に設計されていて、そして支配的な方法であるにもかかわらず 彼らにとっては、IMアルゴリズムはMontezumaのRevengeやPitfallとも戦っています。 IMなしのアルゴリズムよりも優れています。 Montezumaの復讐については、これまでのところ最高のそのようなアルゴリズム 周り11,500 17,500【16の最大値と平均値 、 39]。 ゲームのレベル1を10％で解決した その実行のうち [16 ]。 IMを使用しても、Pitfallでアルゴリズムスコアが0を超えることはありません（確率テストで）。 人間のデモンストレーションなしの環境、）。 我々は、脱離と脱線があると仮定します。 IMアルゴリズムがうまく機能しない主な理由。 提供しやすいドメインの知識を活用する場合は、MontezumaのRevengeスコアでGo-Exploreを実行してください。 平均666,474、その最高の実行スコアはほぼ1,800万で、1,441のレベルを解決します。 落とし穴、 Go-Exploreのスコアは平均59,494で、最大107,363です。これは最大値に近いです。 4
5ページ
112000ポイントのゲームの。 ドメインの知識を利用しなくても、Go-Exploreは依然としてaを獲得します。 モンテスマの復讐の43,763の平均。 すべてのスコアは以前のものより劇的に改善されています 最先端。 ゲームを解くことと最先端の技術を生み出すことに関するこれと他のすべての主張 テストでは確率論が必要であるが、決定論的訓練は許容されるとスコアは仮定する （セクション 2.1.3で 説明されてい ます ）。 Go-Exploreは解くための有望な新しいアルゴリズムであると結論します。 まばらなおよび/または欺瞞的な報酬を用いた困難な探査のRLタスク。 2 Go-Exploreアルゴリズム 覚えていて有望な国家に確実に戻ることは、効果的であるために不可欠であるという洞察 疎報酬問題の探索は、Go-Exploreの中核です。 この洞察はとても柔軟だから Go-Exploreは事実上、一連のアルゴリズムを網羅しています。 この重要なアイデアを中心に構築されています。 この論文の実験のために実行され、記述された変種 このセクションの詳細は、2つの異なるフェーズに依存しています。 それが標準的なデモンストレーションを提供している間 Go-Exploreによって開かれた可能性の中で、他の変種もまた議論されています（例えばセクション 4）。 将来のアプリケーションのためにより広いコンパスを提供してください。 2.1フェーズ1：解決するまで探索する このホワイトペーパーで紹介するGo-Exploreの2フェーズ版では、フェーズ1の目的は以下のとおりです。 状態空間を調べ、後でロバストに変換できる1つ以上の高性能軌道を見つけます。 そのために、フェーズ1では、興味深いさまざまなゲーム状態のアーカイブを作成します。 私達は「細胞」（ 2.1.1 節 ）とそれを導く軌跡を呼びます。 それはアーカイブから始まります 開始状態を含みます。 そこから、次の手順を繰り返してアーカイブを構築します。 現在のアーカイブ（ 2.1.2 節 ） からセルを選択し、確率を追加せずにそのセルに戻る 探査し（ 2.1.3 節 ） 、その場所から確率的に探索する（2.1.4節）。 の間に このプロセス、新たに遭遇した細胞（およびそれらに到達する方法）、または改善された軌跡 既存のセルはアーカイブに追加されます（セクション 2.1.5） 。 2.1.1セル表現 理論的には、Go-Exploreを高次元の状態空間（各セルに直接配置）で実行することができます。 厳密に1つの状態を含みます。 しかし、そうすることは実際には手に負えないでしょう。 扱いやすい Atariのような高次元の状態空間、Go-Exploreのフェーズ1では、低次元の空間が必要です。 検索対象の範囲内（ただし、この場合も最終的なポリシーは元の同じ状態空間で機能します） ケースピクセル）。 このように、セル表現は、状態を混同しないように、類似の状態を混同すべきです。 それは意味があります。 このように、良いセル表現は観測値の次元数を 意味のある低次元空間 豊富な文献が良い表現を得る方法を調査します ピクセルから。 1つの選択肢は、で訓練されたニューラルネットワークの中間から潜在コードを取得することです。 必要に応じて補助を追加して、外因性および/または内因性の動機を最大化する従来のRLアルゴリズム 報酬の予測などのタスク [52 ]。 その他のオプションには、以下のような教師なし技法が含まれます。 自動符号化[53 ]または将来の状態を予測するネットワーク 、およびピクセルなどのその他の補助的なタスクとして コントロール[ 54] 。 今後の作業でこれらのテクニックの一部または全部をGo-Exploreでテストすることは興味深いことですが、 Go-Exploreを使ったこれらの最初の実験では、2つの異なる表現を使ってそのパフォーマンスをテストしました。 ゲーム固有のドメイン知識を利用していない単純なもの、および悪用しているもの 提供しやすいドメインの知識。 ドメイン知識のないセル表現 我々は、非常に単純な次元数減少手順が、次のものに関して驚くほど良好な結果をもたらすことを見出した。 モンテズマの復讐 主な考え方は、現在のゲームフレームを単純にダウンサンプリングすることです。 具体的には、 （1）各ゲームフレームの画像をグレースケールに変換します（2）それを面積のある11×8の画像に縮小します 補間（すなわち、ダウンサンプリングされた画素の領域内の平均画素値を使用）、（３）再スケーリング 元の0〜255ではなく、0〜8の整数になるようにピクセル強度を調整します（図 3） 。 縮小寸法および画素強度範囲は、グリッドサーチにより見出された。 積極的な 5
6ページ
図3：ドメインの知識がない場合のセル表現の例 各ゲームフレームをサンプリングします。 完全に観察可能な状態、カラー画像はグレースケールに変換され、 8つの可能なピクセル強度で11×8の画像に縮小しました。 この表現で使用されているダウンスケーリングは、BellemareらによるBasic機能セットを彷彿とさせます。 [ 30] 。 このセル表現はゲーム特有の知識を必要とせず計算が速いです。 ドメイン知識を持つセル表現 提供しやすいドメイン知識を統合するアルゴリズムの能力は、重要な資産になる可能性があります。 Montezumaの復讐では、ドメイン知識はx、y位置のユニークな組み合わせとして提供されます （各セルが16×16ピクセルのグリッドに離散化された）エージェントの部屋番号、レベル番号、 そしてどの部屋に現在保持されている鍵が見つかったか。 Pitfallの場合、x、y位置のみ エージェントの数と部屋番号が使用されました。 この情報はすべてピクセルから直接抽出されたものです。 主人公の位置などのオブジェクトを検出するための簡単な手書きの識別器を組み合わせたもの 2つのゲームでの地図の構造についての私達の知識を使って（付録 A.3 ）。 ゴー探索中 フェーズ1のセル表現でドメイン知識を活用する機会を提供します。 フェーズ2で作成された強固なニューラルネットワークは、まだピクセルのみから直接再生されます。 2.1.2セルを選択する フェーズ1の各反復では、探索対象のセルがアーカイブから選択されます。 この選択は 一様にランダムに作成されますが、多くの場合、作成することでそのベースラインを改善できます（または あるセルを他のセルよりも優先させるための発見的方法。 予備実験では、 そのような発見的方法は、一様ランダムサンプリングよりも性能を向上させることができる（データは示さず）。 の 正確なヒューリスティックは解決される問題によって異なりますが、高レベルでは、私たちのヒューリスティックは 仕事はより有望であると考えられる細胞のためにより高い正の重みを各細胞に割り当てる。 にとって たとえば、セルは頻繁にはアクセスされていないため、最近使用されているため、セルが優先される場合があります。 新しい細胞を発見すること、または発見されていない細胞の近くにいると予想されること。 すべてのセルの重みは 各セルが次に選択される確率を表すように正規化されます。 セルに重みが与えられることはありません 0に等しいので、原則としてすべてのセルがさらなる探索のために利用可能なままです。 正確なヒューリスティック 我々の実験の結果は付録 A.5に 記載されてい ます。 2.1.3決定論的シミュレータを活用するためのセルと機会への回帰 Go-Exploreの主な原則の1つは、追加の探索なしに有望なセルに戻ることです。 そのセルから探索する前に。 Go-Exploreの哲学は、私たちがに戻るべきだということです。 問題の制約を考えると、そのセルはできるだけ簡単にできます。 に戻る最も簡単な方法 セルは、世界が決定論的でリセット可能であるかどうかで、シミュレータの状態をリセットすることができます。 そのセルへの前回の訪問へ。 そのようなリセットを実行することがRL研究にとって許容できるかどうかは Go-Exploreの最初の発表に動機付けられた興味深い議論の主題[ 55] 。 決定論を利用し、そのようなリセットを実行する能力は、2つの要素があることを認識するように強制します。 RLアルゴリズムを使って解決したい問題の種類： テスト時間のみ、およびテストとトレーニングの両方で確率論を必要とするもの。 私たちは前者から始めます。 現在のRLアルゴリズムは安全でない行動をとる可能性があり[ 56、57 ]、それを必要とするためです。 経験の途方もない量学習する[41 、 42、58]、中RLの大多数のアプリケーション 近い将来には、（そしてオプションとして）移管される前にシミュレータでのトレーニングが必要になるでしょう。 現実の世界に微調整します。 たとえば、ロボット工学の学習アルゴリズムを使用した作業のほとんどは、 現実世界に解決策を移す前にシミュレータ。 それは直接学習するからです ロボットは遅く、サンプル効率が悪く、ロボットを損傷する可能性があり、危険である可能性があります[ 59–61 ]。 幸いなことに、 6
7ページ
多くの分野で、シミュレータが利用可能です（例：ロボットシミュレータ、交通シミュレータなど）。 洞察 Go-Exploreの利点は、そのようなシミュレータを決定論的にすることができるという事実を利用できることです。 特に困難な探査の問題でパフォーマンスを向上させるため。 多くの種類の問題に対して、我々は 信頼できる最終的な解決策（例えば、自然災害の後に確実に生存者を見つけるロボット）を求め、 最初に決定論的な訓練によってこの解を得るかどうかを気にする原則的な理由はありません。 評価時に確率的なものも含め、これまで解決できなかった問題を解決できるかどうか（テスト） 時間をかけて、シミュレータを決定論的にすることで、この機会を利用するべきです。 シミュレータが利用できない場合や学習アルゴリズムが直面しなければならない場合もあります トレーニング中の確率 この2番目のタイプの問題に対するアルゴリズムを作成しテストするために、 決定論と再設定可能性を利用する。 この種の問題の例としては、次のような場合があります。 現実の世界で直接学ぶ（そして効果的なシミュレータは利用できず、学ぶことができない） または私たち自身を含む生物学的動物の学習を研究するとき。 私たちはGo-Exploreを信じています トレーニング目標付きの方針[62により、このような状況を扱うことができ 、確実に細胞に戻る63] 探査段階中のアーカイブにあります。これは将来の研究にとって興味深い分野です。 しながら 計算上はるかに高価で、この戦略は最後に完全に訓練された方針をもたらすでしょう つまり、探査段階では、最後に強固化段階は必要ありません。 我々 環境がそれを妨げる確率論の形態を持っているところにいくつかの問題があることに注意してください エージェントがどのアクションを実行するかにかかわらず、アルゴリズムは確実に特定のセルに戻ることができません（例： ポーカーでは、2つのエースがある状態にあなたを確実に導く一連の行動はありません。 Go-Exploreが将来の作業のためにその問題設定に役立つかどうかについての議論と研究を残します。 この区別を念頭に置いて、今度はMontezumaのRevengeとPitfallがそれを表しているのかどうかを尋ねることができます。 最初のタイプのドメイン（ここで私たちが気にするのはテスト時の確率論に対して頑健な解決策です） 2番目（トレーニング中にアルゴリズムが確率性を処理する必要がある状況）。 私たちはほとんど信じない Go-Exploreに関する最初のブログ投稿 [55 ]の前に、コミュニティの人々はこの質問を考慮していました 。 そしてそれはこの問題に関して健全な議論を引き起こしました。 Atariゲームは問題の代理人だから 私達はRLで解決したいと思っています、そして両方のタイプの問題が存在するので、当然の結論は それぞれにベンチマークがあるはずです。 あるバージョンのタスクでは、テスト中にのみ確率を要求することができます。 もう1つは、トレーニングとテストの両方で確率論を必要とする可能性があります。 この中のすべての結果と主張 このホワイトペーパーのバージョンは、これらのドメインのバージョン用です。 訓練（すなわち、確率論は評価の間だけ必要とされる）。 トレーニング中にGo-Exploreを適用する 確率論は近い将来にわたる刺激的な研究の道であり続けている。 私たちが気にしているのはテスト時に信頼できるポリシーであることが重要な洞察です。 Go-Exploreは、最初に問題を解決し（フェーズ1）、次に（必要に応じて）対処できるということです。 ソリューションを後でより堅牢にする（フェーズ2）。 決定論の通常の見方とは対照的に、 堅牢で高性能の薬剤を製造するための障害となるのは、それを味方にすることです。 探査中に、その後解決策はその後のロバスト化を介して非決定論へと拡大した。 そのような洞察が役立つことができる重要な領域は、トレーニングがしばしば行われるロボット工学です。 政策が現実の世界に移される前のシミュレーション [ 59–61]。 このホワイトペーパーの実験では、確定的トレーニングを利用しているため、セルに戻ることができます。 それにつながる一連のアクションを保存し、その後それらのアクションを再生することによって。 しかしながら、 単にこの一連のステップに加えて、エミュレータの状態を保存し、その状態を復元するだけです。 セルを再訪問すると効率が上がります。 そうすることで、必要なステップ数が減りました 少なくとも1桁の大きさでシミュレートされる（付録 A.8） 。 Go-Exploreの現在のバージョンは、決定的な設定で動作しているという事実のために フェーズ1では、各セルは、与えられたセルにつながる命令の開ループシーケンスに関連付けられています。 初期状態。任意の状態をアクションにマップする適切なポリシーではありません。 真の政策は フェーズ2（セクション 2.2）での ロバスト化 。 2.1.4細胞からの探索 セルに到達したら、任意の探索方法を適用して新しいセルを見つけることができます。 この作品ではエージェント 繰り返しの95％の確率で、k = 100のトレーニングフレームに対してランダムな行動をとることによって探索する 各トレーニングフレーム（エージェントがアクションをとることを許可されているフレーム）での前のアクション したがって、フレームスキップのためにスキップされたフレームは含まれません 。 付録 A.1を 参照してください 。 に達する以外に 探査のためのk = 100トレーニングフレームの制限、探査はエピソードの終わりにも中止されます（定義済み 付録 A。2）では 、エピソードの終了につながったアクションは生成されないため無視されます。 宛先セル 7
8ページ
興味深いことに、そのような探査はニューラルネットワークや他のコントローラを必要としません。 本稿の実験では、探索段階（フェーズ1）にニューラルネットワークを使用した。 （フェーズ2までニューラルネットワークを訓練しません）。 完全にランダムな探査がうまくいくという事実 さらに探求する前に有望な細胞に単純に戻ることの驚くべき力をよく強調している、 インテリジェントに（例えば訓練された方針を介して）探求することは私達の結果を改善する可能性が高いと我々は信じているが そしてそれは将来の仕事のための興味深い道です。 2.1.5アーカイブを更新する エージェントがセルから探索している間、アーカイブは2つの条件で更新されます。 最初の条件は エージェントがアーカイブ内にまだないセルを訪問したとき 与えられたセルから探索する。 この場合、そのセルは4つの関連した断片でアーカイブに追加されます。 メタデータ：（1）エージェントがどのようにそのセルにたどり着いたか（ここでは、開始状態からそれまでの完全な軌跡） （2）セルを発見した時点の環境の状態（環境がサポートしている場合） このような操作は、このホワイトペーパーの2つのAtariゲームドメインに当てはまります。（3）累積 その軌跡のスコア、および（4）その軌跡の長さ。 2番目の条件は、新しく遭遇した軌跡がaに属する軌跡よりも「良い」場合です。 セルはすでにアーカイブに入っています。以下の実験では、新しい軌道をより良いものとして定義します。 新しい軌跡の累積スコアが高い場合、または新しい軌跡の累積スコアが高い場合は、既存の軌跡 同じスコアのより短い軌跡。どちらの場合でも、アーカイブ内の既存のセルは更新されます。 新しい軌跡、新しい軌跡の長さ、新しい環境の状態、および新しいスコアで。 に さらに、このセルが選択される可能性に影響を与える情報（付録 A.5を参照）がリセットされます。セルが選択された合計回数とセルが選択された回数を含む 別の細胞の発見につながって以来、選ばれました。これらの値をリセットすると、 セルに到達するための新しい方法は実際にはより多くの可能性があるため、セルはさまざまな状態を混同します。 そこから探検するための有望な足がかりです（それで我々はその選択を奨励したいです）。リセットしません セルが最近アクセスされたためにアクセスされた回数を記録するカウンタ 最近更新されたセルおよび最近発見されたセルと区別がつかない発見されたセル 訪問数が少ない人は、探査することがより有望です。 私たちの拡大する知識の範囲。 セルは多くの状態を融合しているので、開始状態Aからセルまでの軌跡を仮定することはできません。 AからBに到達するための別のより良い方法を代用すると、セルCからBはCに到達します。したがって、 セルに到達するためのより良い方法は、その上に構築された他のセルの軌跡には組み込まれていません。 元の軌跡 ただし、このような置換を実行すると、目標条件付きでもうまくいく場合もあります。 そうでなければ堅牢なポリシーであり、その可能性を調査することは将来の作業のための興味深い道です。 2.1.6バッチ実装 複数のCPUを利用するためにフェーズ1を並列に実装しました 22個のCPUコアを持つ単一のマシン）：各ステップで、bセルのバッチが選択されます（置き換えあり）。 セクション 2.1で説明されている規則に従って。2と付録A.5、およびそれぞれからの調査これらの細胞はそれぞれ並行して進行する。より多くのインスタンスを実行するために複数のCPUを使用することに加えて 環境によっては、高いbはセル選択の確率を再計算する頻度を減らして時間を節約します。 この計算はアーカイブとして実行時間のかなりの部分を占めるため、これは重要です。 大きくなる（ただし、後者の要因は将来他の方法で軽減される可能性がある）。サイズだから of bは、Go-Exploreの探索動作にも間接的な影響を及ぼします（たとえば、最初の 最初の反復でstateはb回選択されることが保証されています）、実際にはハイパーパラメーターです。 その値は付録 A.6にあります。 2.2フェーズ2：厳密化 成功した場合、フェーズ1の結果は1つ以上の高性能軌道です。ただし、フェーズ1の場合 Go-Exploreのシミュレータでの決定論を活かした、そのような軌跡は誰にも頑強ではないでしょう テスト時に存在する確率性。フェーズ2では、このギャップに対処するために、 模倣学習によるノイズ。デモンストレーションからの学習（LfD）とも呼ばれます。重要なのは、確率論 最終的な政策がそれが直面する確率に頑健であるように、フェーズ2の間に追加されます。 テスト環境での評価 したがって、訓練されているポリシーは、模倣する方法を学ぶ必要があります Go-Explore探査フェーズで得られた軌道と同様に実行します。 8
9ページ
同時に元の軌跡には存在しなかった状況に対処する。 による 環境の確率論上、この調整は非常に困難な場合がありますが、それでも 最初から疎報酬問題を解決しようとするよりはるかに簡単です。 ほとんどの模倣学習アルゴリズムはフェーズ2に使用できますが、異なる種類の模倣 学習アルゴリズムは、結果として得られるポリシーに定性的な影響を及ぼす可能性があります。密接に試みるLfDアルゴリズム デモンストレーションの動作を模倣することで、それを改善するのに苦労することがあります。このため、私たちは そのデモンストレーションを改善することができることが示されているLfDアルゴリズムを選びました： SalimansとChenによる逆方向アルゴリズム[28]。 最後の状態近くでエージェントを起動することで機能します。そして、そこから通常のRLアルゴリズムを実行します（この場合はProximal Policy）。 最適化（PPO）[64 ]。アルゴリズムが以下と同じかそれ以上の報酬を得ることを学んだ後軌道の終点近くの出発地からの軌跡の例では、アルゴリズムは エージェントの出発点を軌跡に沿って少し前の位置まで移動し、このプロセスを繰り返します。 最終的にエージェントは例の軌跡以上のスコアを得ることを学びました 初期状態からの道 似たようなアルゴリズムが独立して発見されたことに注意してください Resnick等による同じ時間。[ 65]。このロバスト化へのアプローチは、エキスパートの軌跡を効果的にカリキュラムとして扱います。 エージェント、ポリシーはそれ自身のスコアを最大化するために最適化されているだけで、実際には正確に強制されていません 軌道を模倣します。このため、このフェーズでは、エキスパートの軌跡をさらに最適化することができます。 それらを超えて一般化します。どちらも我々が実験で実際に観察したものです（第 3 節）。元の軌跡よりも高いスコアを求めることに加えて、それは 後に集まったものよりも短期の報酬をより多く賞する割引率、それはまた圧力を持っています それが報酬を集める効率を改善するため。したがって、元の軌跡が 不必要な行動（行き止まりを訪れて戻ってくるなど）では、そのような行動は 強奪（私たちも観察した現象）。 2.3追加の実験と分析の詳細 AtariゲームでトレーニングされたRLアルゴリズムのサンプルの複雑さを比較するのは、次の理由により面倒です。 フレームスキップの一般的な使い方[ 31、66 ]、ポリシーはn番目（ここでは4）ごとに見て行動するだけです。そして、その動作は、ポリシーを実行する計算を節約するために介在するフレームに対して繰り返される。 具体的には、スキップされたフレームがカウントされるかどうかは曖昧になる可能性があります（これを呼び出します）。 サンプルの複雑さについては、「ゲームフレーム」または無視されます（これを「トレーニングフレーム」と呼びます）。 に この作品は、私たちは常にそれに応じて「フレーム」という言葉を修飾し、私たちが報告するすべての数字は測定されます ゲームフレームで。付録A.1 はこの問題の微妙な点についてさらに詳しく述べています。Atariゲームはデフォルトで決定論的であるため、何らかの形の確率論は 確率的テスト環境を提供するために導入されました。これはAtariを有益なものにするために望ましいものです。 RLアルゴリズム用テストベッド これまでの研究に続いて、Atariに確率論を導入します。 これまでに採用された2つのテクニックを使った環境：ランダムな操作とスティッキーなアクション。 ランダムなno-opsは、エージェントが最大30のno-ops（何もしないコマンド）を実行することを強制されることを意味します。 ゲームの開始 ほとんどのAtariゲームは、危険が存在するかどうかに影響するタイマーで動作するため ランダムな数のノーオペレーションを取っているかどうか、またはさまざまな危険、アイテム、または敵がいる場所 毎回、世界はわずかに異なる状態になります。 Go-Explore Phase 1）で見つかったものは機能しなくなります。Random no-opsはによって最初に導入されました。 Mnihら。 [3 ]、そしてそれらはその後のほとんどの論文で確率論の主要な源として採用された。アタリドメインで働い[3 、 26、27、34、38、41、42、45、67-73]。ランダムな操作は、単一の記憶された軌跡がAtariゲームを解決するのを妨げますが、残りは ゲームの意味は決定論的なままです。つまり、利用できる決定論はまだたくさんあります。 他のいくつかの形式の確率論が提案されているが（例えば、人間は再開する [74 ]、ランダムフレームをスキップする[ 75]など、特にエレガントな形式はスティッキーアクションです[31]。各ゲームフレームで新しく選択したものを実行する代わりに、前のアクションを繰り返す可能性があります。 アクション。 確率論を導入するこの方法は、人間がどのように完全な枠組みではないかに似ていますが、保持することができます。 意図したより少し長い間、またはボタンを押すのが少し遅れる可能性があるボタン。 Atariゲームは人間の遊びのために設計されているので、一般的にスティッキーアクションの追加は ゲームが解決可能になるのを妨げるわけではありません、そしてそれはゲーム内のすべての状態にいくらかの確率を加えます。 ゲームではなく、スタートだけではありません。私たちの最初のブログ投稿 [55 ]には、ランダムな操作手順しか含まれていませんでしたが、私たちのrobustificationとすべてのpost robustificationのテスト結果は組み合わせて生成されます 9
10ページ
ランダムなノーオペレーションとスティッキーアクションの両方。セクション 3とで比較するすべてのアルゴリズム付録A .9も同様に、何らかの形式の確率（no-op、stickyの形式）でテストされました。ただし、Go-Exploreとは異なり、注目に値します。 ほとんどの人は、トレーニング中に確率論を処理しなければなりませんでした。でテストされた関連アルゴリズム 決定論的環境についてはセクション 5で説明します。 すべてのハイパーパラメータは、実験ごとに個別のグリッド検索を実行することによって見つかりました。 の 最終的な、 最高性能のハイパーパラメータは、付録A.6の表1と2にリストされています。与えられた区間は、ピボタルを使って計算された95％ブートストラップ信頼区間です。 10,000回の再標本化によって得られた経験的）方法[ 76]。信頼区間はと報告されます次の表記法：stat（CI：lower - upper）ここで、statは統計値です（特に明記しない限り、平均値）。 指定）。影付きの領域を含むグラフでは、これらの領域はブートストラップされた95％パーセンタイルを示します。 平均の信頼区間。1000回リサンプリングして得られます。探査段階のグラフ （フェーズ1）は、ほぼ4Mのゲームフレームごとのデータと、ロボット化フェーズのグラフを示しています。 （フェーズ2）は、およそ13万ゲームフレームごとのデータを示しています。 解決策を見つけた後でも、ロバスト化プロセスは分岐する可能性があるため、ニューラルネットワークは たとえ高性能のソリューションがあったとしても、トレーニングの最後には必ずしもうまく機能しません。 このプロセス中のある時点で見つかりました。よく機能するニューラルネットワークを検索する それが発見されたときより少ない、すべてのロバスト化実行（フェーズ2）は神経のチェックポイントを作り出しました およそ13Mのゲームフレームごとにネットワーク。パフォーマンス値は、 強要は騒々しい、私たちはそれらのパフォーマンスから最高のパフォーマンスのチェックポイントを選択することはできません。 値だけ。そのため、各ロバスト化実行の最後に、最も低いチェックポイントのうち、 max_starting_point（またはそれに近い）、チェックポイントのランダムなサブセット（10から50） そのチェックポイント内に保存されているニューラルネットワークのパフォーマンスを評価するためにテストされています。私達はテストします ロバスト化の実行は通常、より成功したチェックポイントを生成するので、ランダムサブセット 現実的にテストします。各実行の最高得点のチェックポイントは、それを考慮して再テストされました。 最良のチェックポイントの選択に固有の選択バイアス。この最後の再テストからのスコアはそれです 報告します。 各チェックポイントからのニューラルネットワークは、ランダムな操作とスティッキーなアクションを使って評価されます。 ３１の可能な開始ｎｏｐ（０から３０までを含む）のそれぞれについて少なくとも５つのスコアが得られる。 の その後、各操作の平均スコアが計算され、チェックポイントの最終スコアが総平均値になります。 個々のノーオペレーションスコアの。特に指定がない限り、デフォルトの制限時間は400,000ゲーム OpenAI Gym [75 ] によって課されたフレームが適用されます。 3の結果 3.1モンテスマの復讐 3.1.1セル表現にドメインの知識がない 0.0 0.5 1.0 ゲームフレーム 1e9 0 10年 20 30 見つかった部屋 最先端 （a）見つかった部屋の数 0.0 0.5 1.0 ゲームフレーム 1e9 0 5,000 10,000 15,000 見つかったセル （b）見つかったセル数 0.0 0.5 1.0 ゲームフレーム 1e9 0 20,000 40,000 6万 最高のスコア 平均的な人間 ヒューマンエキスパート （c）アーカイブ内の最大スコア 図4：縮小されたフレームをオンにした場合のGo-Exploreの探索フェーズのパフォーマンス モンテズマの復讐 人間とアルゴリズムの最先端技術を示す線は比較のためのものです。 ison、しかしこのプロットのGo-Exploreスコアは決定論的バージョンのゲームに基づいていることを思い出してください。 （このセクションに示されているフェーズ2後のスコアとは異なります）。
